{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from config import gkey, username, password\n",
    "import requests\n",
    "import json\n",
    "import geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/MSP_neighborhoods.csv'\n",
    "MSP_df = pd.read_csv(file_path, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSP_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert age share columns to float and re-order into 5 different age groups\n",
    "MSP_df['AgeBelow18Prct'] = (MSP_df['Under 5 years - Share'].str.rstrip('%').astype('float') / 100.0\n",
    "                              + MSP_df['5-9 years - Share'].str.rstrip('%').astype('float') / 100.0\n",
    "                              + MSP_df['10-14 years - Share'].str.rstrip('%').astype('float') / 100.0 \n",
    "                              + MSP_df['15-17 years - Share'].str.rstrip('%').astype('float') / 100.0)\n",
    "\n",
    "MSP_df['Age18To34Prct'] = (MSP_df['18-24 years - Share'].str.rstrip('%').astype('float') / 100.0\n",
    "                              + MSP_df['25-34 years - Share'].str.rstrip('%').astype('float') / 100.0)\n",
    "\n",
    "MSP_df['Age35To54Prct'] = (MSP_df['35-44 years - Share'].str.rstrip('%').astype('float') / 100.0\n",
    "                              + MSP_df['45-54 years - Share'].str.rstrip('%').astype('float') / 100.0)\n",
    "\n",
    "MSP_df['Age55To75Prct'] = (MSP_df['55-64 years - Share'].str.rstrip('%').astype('float') / 100.0\n",
    "                              + MSP_df['65-74 years - Share'].str.rstrip('%').astype('float') / 100.0)\n",
    "\n",
    "MSP_df['AgeAbove75Prct'] = (MSP_df['75-84 years - Share'].str.rstrip('%').astype('float') / 100.0\n",
    "                              + MSP_df['85 years and older - Share'].str.rstrip('%').astype('float') / 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set percent string fields to float fields\n",
    "MSP_df['PublicTransportPrct'] = MSP_df['Public transportation - Share'].str.rstrip('%').astype('float') / 100.0\n",
    "MSP_df['WalkBiketoWorkPrct'] = MSP_df['Walked, biked, worked at home, or other - Share'].str.rstrip('%').astype('float') / 100.0\n",
    "MSP_df['UnemploymentPrct'] = MSP_df['Unemployment rate - Share'].str.rstrip('%').astype('float') / 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select columns to be used in our database\n",
    "MSP_df_reduced = MSP_df[[\"Neighborhood\", \"City\", \"Total population\",\n",
    "    \"Total households\", \"Median household income (2008-2012, 2012 dollars)\", \n",
    "    \"PublicTransportPrct\", \"WalkBiketoWorkPrct\",\n",
    "    \"UnemploymentPrct\", \"AgeBelow18Prct\", \"Age18To34Prct\", \"Age35To54Prct\",\n",
    "    \"Age55To75Prct\", \"AgeAbove75Prct\"]]\n",
    "MSP_df_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename MSP_df columns to match target database\n",
    "neighborhood_df = MSP_df_reduced.rename(columns={\"Total population\": \"NeighborhoodPopulation\", \n",
    "                       \"Total households\": \"NeighborhoodHouseholds\",\n",
    "                      \"Median household income (2008-2012, 2012 dollars)\": \"MedianIncome\"})\n",
    "neighborhood_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the center of each neighborhood in Minneapolis \n",
    "# by averaging boundary coordinates\n",
    "neighborhoodName = []\n",
    "lats = []\n",
    "lngs = []\n",
    "avgLats = []\n",
    "avgLngs = []\n",
    "with open('data/Minneapolis_neighborhoods.geojson') as f:\n",
    "    gj = geojson.load(f)\n",
    "    \n",
    "    # grab features from each neighborhood\n",
    "    for i in gj['features']:\n",
    "        \n",
    "        #grab neighborhood name\n",
    "        neighborhood = i['properties']['BDNAME']\n",
    "        neighborhoodName.append(neighborhood)\n",
    "        \n",
    "        #clear lat and lngs lists for each loop\n",
    "        lats.clear()\n",
    "        lngs.clear()\n",
    "        \n",
    "        #grab coordinates that make up neighborhood bounds\n",
    "        coordinates = i['geometry']['coordinates'][0][0]\n",
    "        for i in coordinates:\n",
    "            lats.append(i[1])\n",
    "            lngs.append(i[0])\n",
    "        \n",
    "        #calculate average coordinates from list of coordinates and store\n",
    "        avgLat = sum(lats) / len(lats)\n",
    "        avgLng = sum(lngs) / len(lngs)\n",
    "        avgLats.append(avgLat)\n",
    "        avgLngs.append(avgLng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the above process for St. Paul\n",
    "with open('data/StPaul_neighborhoods.geojson') as f:\n",
    "    gj = geojson.load(f)\n",
    "    \n",
    "    # grab features from each neighborhood\n",
    "    for i in gj['features']:\n",
    "        \n",
    "        #grab neighborhood name\n",
    "        neighborhood = i['properties']['name2']\n",
    "        neighborhoodName.append(neighborhood)\n",
    "        \n",
    "        #clear lat and lngs lists for each loop\n",
    "        lats.clear()\n",
    "        lngs.clear()\n",
    "        \n",
    "        #grab coordinates that make up neighborhood bounds\n",
    "        coordinates = i['geometry']['coordinates'][0][0]\n",
    "        for i in coordinates:\n",
    "            lats.append(i[1])\n",
    "            lngs.append(i[0])\n",
    "        \n",
    "        #calculate average coordinates from list of coordinates and store\n",
    "        avgLat = sum(lats) / len(lats)\n",
    "        avgLng = sum(lngs) / len(lngs)\n",
    "        avgLats.append(avgLat)\n",
    "        avgLngs.append(avgLng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_center_dict = {\"Neighborhood\" : neighborhoodName,\n",
    "                            \"Latitude\" :  avgLats,\n",
    "                            \"Longitude\" : avgLngs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_coord_df = pd.DataFrame(neighborhood_center_dict)\n",
    "center_coord_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of place types we will be looking for within google places\n",
    "typeList = ['supermarket', 'park', 'gym', 'restaurant', 'school', 'transit_station', 'church']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary for storing data\n",
    "places = {\n",
    "    \"placeName\": [],\n",
    "    \"placeType\": [],\n",
    "    \"placeLat\": [],\n",
    "    \"placeLng\": []\n",
    "}\n",
    "places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in typeList:\n",
    "    # parameters for api call\n",
    "    params = {\n",
    "        \"radius\": 3000,\n",
    "        \"type\": type,\n",
    "        \"key\": gkey\n",
    "    }\n",
    "\n",
    "    #base url\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
    "\n",
    "    # use iterrows to iterate through pandas dataframe\n",
    "    for index, row in center_coord_df.iterrows():\n",
    "        lat = row['Latitude']\n",
    "        long = row['Longitude']\n",
    "        location = f\"{lat}, {long}\"\n",
    "\n",
    "        # add keyword to params dict\n",
    "        params['location'] = location\n",
    "\n",
    "        # assemble url and make API request\n",
    "        places_data = requests.get(base_url, params=params).json()\n",
    "        places_info = places_data['results']\n",
    "        \n",
    "        # iterate through results to pull out the name, lat, and long of found places\n",
    "        for i in places_info:\n",
    "            places['placeName'].append(i['name'])\n",
    "            places['placeType'].append(type)\n",
    "            places['placeLat'].append(i['geometry']['location']['lat'])\n",
    "            places['placeLng'].append(i['geometry']['location']['lng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_df = pd.DataFrame(places)\n",
    "places_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete duplicate values which may have been grabbed by overlapping radius calls\n",
    "places_no_dup_df = places_df.drop_duplicates()\n",
    "places_no_dup_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "connection_string = f\"{username}:{password}@localhost:5432/MSP_Neighborhoods\"\n",
    "\n",
    "# Create the engine\n",
    "engine = create_engine(f'postgresql://{connection_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking connection by finding table names\n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \"to_sql\" function to load all transformed dfs' data into postgres\n",
    "\n",
    "# Neighborhoods\n",
    "neighborhood_df.to_sql(name='Neighborhoods', con=engine, if_exists='replace', index=True, index_label='NeighborhoodID')\n",
    "\n",
    "# Places\n",
    "places_no_dup_df.to_sql(name='Places', con=engine, if_exists='replace', index=True, index_label='placeID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
